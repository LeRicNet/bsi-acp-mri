import argparse
import datetime
import numpy as np
import os
import ray
from sklearn.model_selection import KFold
import tensorflow as tf
import tensorflow_addons as tfa

# Local Imports
from Bayes.src.data_manager import ACPMRILite
from Bayes.src.swag2 import SWAG


# Argument Parser
def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", choices=['MRI', 'CT'], default='MRI',
                        help="String indicating which preloaded dataset to evaluate. [MRI, CT]")
    parser.add_argument("--model_path", type=str,
                        help="Filepath to pretrained model.")
    parser.add_argument("--checkpoint_path", type=str)
    parser.add_argument("--save_path", type=str)
    parser.add_argument("--epochs", type=int,
                        help="Number of epochs to run.")
    parser.add_argument("--lr_init", type=float,
                        help="Initial learning rate.")
    parser.add_argument("--wd", type=float,
                        help="Weight decay")
    parser.add_argument("--momentum", type=float, default=0.9,
                        help="momentum parameter for optimizer.")
    parser.add_argument("--swag_start", type=int,
                        help="Number of epochs after which to start SWAG")
    parser.add_argument("--swag_interval", type=int,
                        help="Number of epochs to evaluate SWAG over")
    parser.add_argument("--swag_lr", type=float,
                        help="Learning rate for SWA")
    parser.add_argument("--max_num_models", type=int,
                        help="Max number of models; Upper bound of the dimensionality of PCA subspace.")
    parser.add_argument('--subspace', choices=['covariance', 'pca', 'freq_dir'], default='covariance')
    parser.add_argument("--seed", type=int, default=42,
                        help="Random seed")

    args, _ = parser.parse_known_args()
    return args

# Main Execution Script
@ray.remote(num_gpus=1)
def ray_run(args, train_idx, test_idx, k, current_time):

    # Set Random Seed
    # NOTE: Even by setting the random seed we still observe ambiguity in the performance.
    tf.random.set_seed(args.seed)

    # load data
    print(train_idx, test_idx)
    # Load Data
    if args.data == "MRI":
        data = ACPMRILite()
        data.test_data_dir = None
        data.batch_size = 2
        data.train_idx = train_idx
        data.test_idx = test_idx
        data.max_n = 500  # this is max N per class...
        train, test = data.load_data_from_files()
        # data.load()

    elif args.data == "CT":
        raise ValueError("CT dataset not currently implemented.")

    # Load Base Model (i.e., graph)
    base_model = tf.keras.models.load_model(args.model_path)
    # Load weights from checkpoint
    base_model.load_weights(args.checkpoint_path)

    # Initial AUPR - checking to see that the loaded model is performing as expected from the given checkpoint
    init_aupr = tf.keras.metrics.AUC(curve='PR')
    for Xt, yt in test:
        init_aupr.update_state(yt, base_model(Xt, training=False))
    print('\n\t** Initial AUPR: {:.4f} **\n'.format(init_aupr.result()))

    # ------------------------------------------------------------------------------------------------------
    # SWAG model
    # ------------------------------------------------------------------------------------------------------
    swag_model = SWAG(base_model, args.subspace)

    # ------------------------------------------------------------------------------------------------------
    # Learning Rate Schedule
    # ------------------------------------------------------------------------------------------------------
    # Every 3 swag_intervals, restart the cosine decay. This means that we will collect the model for SWAG
    # three times for each learning rate cycle. These models will roughly be captured at a relative high, mid,
    # and low learning rate.
    # lr = tf.keras.experimental.CosineDecayRestarts(args.lr_init, int(args.swag_interval * 3))
    # swag_lr = tf.keras.experimental.CosineDecayRestarts(args.swag_lr, int(args.swag_interval * 3))
    lr = tfa.optimizers.TriangularCyclicalLearningRate(args.lr_init, args.lr_init * 0.1, int(args.swag_interval * 3))
    swag_lr = tfa.optimizers.TriangularCyclicalLearningRate(args.swag_lr, args.swag_lr * 0.1, int(args.swag_lr * 3))

    # Load the optimizer saved with the model. Currently this is a custom LazyAdamW optimizer
    # generated by extending tfa.optimizers.LazyAdam with tfa.optimizers.DecoupledWeightDecayExtension
    #
    # The original SWAG and Subspace Inference papers use a simple SGD optimizer. I attempted to use the SGD
    # optimizer but did not succeed, and our 2D manuscript identified through genetic algorithm that SGD was
    # not the best choice for creating a classifier for ACP MRI images. Adam optimizers work better in
    # nonconvex optimization problems, so perhaps the loss surface of the current problem is nonconvex?


    # Reconstruct optimizer using configuration updated with values passed to args
    optimizer_config = base_model.optimizer.get_config()
    optimizer_config['learning_rate'] = lr
    optimizer = base_model.optimizer.from_config(optimizer_config)

    # Same for SWAG optimizer but with SWAG parameters
    optimizer_config['learning_rate'] = swag_lr
    swag_optimizer = base_model.optimizer.from_config(optimizer_config)

    # Extend using Stochastic Weight Averaging (SWA)
    optimizer = tfa.optimizers.SWA(
        optimizer,
        start_averaging=args.swag_start)
    swag_optimizer = tfa.optimizers.SWA(
        swag_optimizer,
        start_averaging=args.swag_start)

    # ------------------------------------------------------------------------------------------------------
    # Loss Function
    # ------------------------------------------------------------------------------------------------------
    # The SWAG/Subspace papers use a simple crossentropy. Other loss functions could also
    # be explored and evaluated.
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True)

    # ------------------------------------------------------------------------------------------------------
    # Training Loop
    # ------------------------------------------------------------------------------------------------------
    max_test_aupr = [(0, 0.)]
    epochs_since_max = 0
    sgd_ens_preds = None
    sgd_target = None


    # ------------------------------------------------------------------------------------------------------
    # Summary Stats (Tensorboard)
    # ------------------------------------------------------------------------------------------------------

    train_log_dir = './swaglogs/gradient_tape/' + args.data + "-" + current_time + '/train-{}'.format(k)
    test_log_dir = './swaglogs/gradient_tape/' + current_time + '/test-{}'.format(k)
    train_summary_writer = tf.summary.create_file_writer(train_log_dir)
    test_summary_writer = tf.summary.create_file_writer(test_log_dir)

    # Performance Metrics
    epoch_loss = tf.keras.metrics.Mean()
    epoch_swag_loss = tf.keras.metrics.Mean()
    epoch_train_aupr = tf.keras.metrics.AUC(curve="PR")
    epoch_test_aupr = tf.keras.metrics.AUC(curve="PR")

    for epoch in range(args.epochs):


        # SWAG Update
        if ((epoch + 1) >= args.swag_start and (epoch + 1) % args.swag_interval == 0) or (epoch == 0):


            for X, y in data.train:
                swag_loss = _train_step_with_swag(swag_model, X, y, loss, swag_optimizer)
                epoch_swag_loss.update_state(swag_loss)

            swag_model.collect_model(base_model)
            swag_model.set_swa()

            # SWAG Evaluation
            epoch_swag_aupr = tf.keras.metrics.AUC(curve="PR")
            for Xt, yt, in data.test:
                epoch_swag_aupr.update_state(yt, swag_model(Xt))
            print("SWAG Loss: {:.3f}; AUPR: {:.3f}; Max/Since: ({:.3f}/{})".format(
                epoch_swag_loss.result().numpy(),
                epoch_swag_aupr.result().numpy(),
                max_test_aupr[-1][1],
                epoch - max_test_aupr[-1][0] + 1))

            with train_summary_writer.as_default():
                tf.summary.scalar('swag_aupr', epoch_swag_aupr.result(), step=epoch)
                tf.summary.scalar('swag_loss', epoch_swag_loss.result(), step=epoch)

            if epoch_swag_aupr.result().numpy() > max_test_aupr[-1][1]:
                max_test_aupr.append((epoch + 1, epoch_swag_aupr.result().numpy()))
                epochs_since_max = 0
                swag_model.save_weights(args.save_path + "cp-k{}.cpkt".format(k))

            else:
                epochs_since_max += args.swag_interval

        # Run training iteration
        for X, y in data.train:

            loss_value = _train_step(base_model, X, y, loss, optimizer)

            epoch_loss.update_state(loss_value)
            epoch_train_aupr.update_state(y, base_model(X))

        # Basic Evaluation
        for Xt, yt, in data.test:
            epoch_test_aupr.update_state(yt, base_model(Xt))

        with train_summary_writer.as_default():
            # tf.summary.scalar('learning_rate', lr(epoch).numpy())
            tf.summary.scalar('loss', epoch_loss.result(), step=epoch)
            tf.summary.scalar('train_aupr', epoch_train_aupr.result(), step=epoch)

        with test_summary_writer.as_default():
            tf.summary.scalar('test_aupr', epoch_test_aupr.result(), step=epoch)



        print("Epoch {}/{}: Loss: {:.4f}; TrAUPR: {:.3f}; TeAUPR: {:.3f}".format(
            epoch + 1,
            args.epochs,
            epoch_loss.result().numpy(),
            epoch_train_aupr.result().numpy(),
            epoch_test_aupr.result().numpy(),
            max_test_aupr[-1][1],
            epoch - max_test_aupr[-1][0] + 1)
        )

        if epochs_since_max > 5:
            print("Early Stopping - Max AUPR: {:.3f}".format(max_test_aupr[-1][1]))
            break
    return max_test_aupr[-1][1]

    # Save model architecture
    # swag_model.save(args.save_path)

def run():

    args = get_args()

    # ------------------------------------------------------------------------------------------------------
    # K-Fold Validation
    # ------------------------------------------------------------------------------------------------------
    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    kf = KFold(n_splits=5, random_state=args.seed)

    futures = []
    for k, (train_idx, test_idx) in enumerate(kf.split(np.ndarray((1000, 2048)))):
        futures.append(ray_run.remote(args, train_idx, test_idx, k, current_time))

    kf_auc = ray.get(futures)
    print("Average AUPR = {:.3f} +/- {:.3f}".format(np.mean(kf_auc), np.std(kf_auc)))



@tf.function
def _train_step(model, X, y, loss, optimizer):
    with tf.GradientTape() as tape:
        loss_value = loss(y, model(X))
        grads = tape.gradient(loss_value, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss_value

@tf.function
def _train_step_with_swag(model, X, y, loss, swag_optimizer):
    with tf.GradientTape() as tape:
        loss_value = loss(y, model(X))
        grads = tape.gradient(loss_value, model.trainable_variables)
    swag_optimizer.apply_gradients(zip(grads, model.trainable_variables))
    return loss_value


if __name__ == '__main__':
    ray.init()
    os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"
    os.environ["CUDA_VISIBLE_DEVICES"] = "1,2,3,4,5"
    # run(get_args())
    run()
